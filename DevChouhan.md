Below is a 60-minute interview roleâ€play tailored to Dev Kant Chouhanâ€™s background 
. Youâ€™ll get a time-boxed script, sample prompts (with interviewer â€œMâ€ and candidate â€œCâ€), and an evaluation guide to land on Hire / No Hire.

â³ 60-Minute Interview Agenda
Time	Section	Focus	Lead
0â€“5 min	Intro & Framing	Build rapport, set expectations	M: â€œHi Dev, today weâ€™ll deep-dive into your Go/K8s work, do a system design, then hit some behavioral questions. Ready?â€
5â€“25 min	Technical Deep Dive	Go concurrency & K8s operator internals	M: â€œWalk me through your custom Kubernetes operator: its architecture, how you handled reconciliation loops and thread-safety.â€
25â€“45 min	System Design	ConfigManagement Service	M: â€œDesign a multi-tenant Configuration Management Service that persists settings in Postgres, supports dynamic updates without deploys, and isolates tenants.â€
45â€“58 min	Behavioral & Leadership	Deliver Results, Dive Deep, Ownership	M: â€œTell me about a time you owned a P1 production issue on your installer CLIâ€”how did you troubleshoot and resolve it?â€
58â€“60 min	Wrap-Up & Candidate Q&A	Clarify next steps	M: â€œAny questions for me? Thanks, Dev!â€

ğŸ¤ Sample Role-Play Script
0â€“5 min: Intro & Framing
M:

â€œDev, great to meet you. Weâ€™ll start with a technical deep dive into your Go/Kubernetes work, then a system-design exercise, and finish with behavioral questions around ownership and delivery. How does that sound?â€

C:

â€œSounds perfectâ€”thanks for the overview!â€

5â€“25 min: Technical Deep Dive (Go & K8s Operator)
M:

â€œYou built custom operators to monitor cluster health. Can you walk me through its reconciliation loop from a high level?â€

C:

Describes: operator scaffold â†’ Informer cache â†’ Reconcile() signature â†’ handling Add/Update/Delete events.

M Probes:

Concurrency: â€œHow do you ensure your controllers handle multiple parallel Reconcile calls safely?â€

Rate-Limiting & Backoff: â€œWhat happens when the API server is overloaded? How do you throttle?â€

State Storage: â€œWhere do you store intermediate stateâ€”annotations, ConfigMaps, or external DB?â€

Error Handling: â€œHow do you detect and recover from a failed reconciliation (e.g., Pod crash during update)?â€

(Look for: use of workqueues, mutexes or per-key locking, context deadlines, structured logging.)

25â€“45 min: System Design (Config Management Service)
M:

â€œLetâ€™s design a Configuration Management Service for Pure Storage multi-tenant users. Requirements:

Store arbitrary JSON/YAML configs per tenant

Dynamic reload by downstream services without restart

Strict tenant isolation (no cross-leak)

Audit logs of changes

How would you architect it?â€

C Walkthrough Should Cover:

API Layer (Go + Gin or gRPC): CRUD endpoints, auth via JWT with tenant claims.

Persistence:

Postgres with a configs table: (tenant_id, config_key, payload JSONB, version, updated_at)

Version history stored in an append-only audit table.

Change Propagation:

Event Bus (e.g., NATS or Kafka) to publish ConfigUpdated events per tenant.

Downstream services subscribe to only their tenant topic.

Dynamic Reload:

Services embed a client library that maintains a long-lived WebSocket or watches a Redis pub/sub channel for updates.

Isolation & Security:

Row-level security in Postgres (current_setting('tenant.id')).

mTLS between services for event bus.

Scaling & HA:

API behind Kubernetes Deployment + HPA.

Postgres in primary-replica mode, read-only replicas for audit queries.

M Deep Probes:

â€œHow do you handle schema migrations of the JSON payload?â€

â€œWhat happens if a config update fails to propagate to a subscriber?â€

â€œHow would you rollback a faulty config across thousands of services?â€

45â€“58 min: Behavioral & Leadership
M:

â€œTell me about a time you owned a P1 incident when your installer CLI failed in production.â€

C (STAR) Should Include:

Situation: On-prem installs were failing due to a missing namespace label.

Task: You were on-call and needed to restore installs within 1 hour.

Action: Added schema validation in CLI, rolled out a hotfix under a feature flag, ran backfill job for stuck clusters.

Result: Restored 100% success rate within 45 min and added automated integration tests.

M Probes:

â€œHow did you coordinate with support and SRE teams?â€

â€œWhat metrics or dashboards did you use to detect ongoing failures?â€

â€œWhat post-mortem actions did you take to prevent recurrence?â€

58â€“60 min: Wrap-Up & Candidate Q&A
M:

â€œThanks, Dev. Can you briefly summarize your top two strengths and one area youâ€™d like to improve?â€

C:

Provides concise self-reflection.

M:

â€œGreat. Do you have any questions for me?â€

ğŸ—³ï¸ Decision Guide
Dimension	Strong Signal	Weak Signal
Technical Depth	Deep understanding of Go internals, operator patterns, race-free designs	Vague on concurrency, no locking/queueing details
Design Rigor	Clear multi-tier architecture, event-driven reload, failover/rollback strategies	Skips audit/versioning, naive polling for reload, no tenancy isolation
Ownership & Impact	STAR with metrics & clear impact, cross-team coordination, post-mortems	Generic stories, no measurable outcomes
Problem Solving	Probes failure modes, anticipates edge cases (network, schema drift, replay)	Misses error handling, no rollback or scaling concerns
Communication	Structures answers, asks clarifying questions, diagrams key components	Rambling, doesnâ€™t seek clarity, unclear flow
