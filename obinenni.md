Based on the candidate's resume for the **SDET role**, here‚Äôs a structured **60-minute interview plan** with **20 questions** that assess:

* **Testing fundamentals & domain knowledge**
* **Automation & technical depth**
* **Leadership principles**: Ownership, Deep Dive, Learn and Be Curious

---

## üïë Interview Structure (60 minutes)

| Time      | Section                       | Focus                                           |
| --------- | ----------------------------- | ----------------------------------------------- |
| 0‚Äì5 min   | Introduction & Ice-breaker    | Candidate background & motivation               |
| 5‚Äì25 min  | Testing Fundamentals & Domain | QA principles, automation, system understanding |
| 25‚Äì45 min | Leadership Principles         | Ownership, deep dive, curiosity                 |
| 45‚Äì55 min | Scenario-based Questions      | Problem-solving & real-world depth              |
| 55‚Äì60 min | Wrap-up                       | Candidate questions & feedback                  |

---

## ‚úÖ Interview Questions

### üîπ Testing Fundamentals & Domain Knowledge (10 questions)

1. **Explain your test strategy when automating a new API with no documentation.**
   *Probing:* How do you validate your assumptions? How do you reverse engineer?

2. **How would you test a machine learning pipeline like the one in CognitiveAI Timber?**
   *Probing:* How do you validate ML outputs? What about data dependencies?

3. **How do you approach writing automation tests in `pytest` for large-scale systems?**
   *Probing:* How do you structure fixtures and manage test data?

4. **Describe your approach to regression testing in the OTV pipelines.**
   *Probing:* What checks do you automate vs. do manually?

5. **What is the difference between smoke, sanity, and regression testing? When would you use each?**

6. **How do you perform CLI testing for an enterprise system like NetApp?**
   *Probing:* How do you handle CLI output validation?

7. **Explain how you would test reliability or failure scenarios in a NAS or disaster recovery solution.**

8. **How do you ensure the automation suite remains maintainable over time?**
   *Probing:* How do you refactor tests or framework components?

9. **How do you decide what test cases to automate first in a new module?**

10. **Describe a scenario where a bug escaped your automation tests. What did you learn from it?**
    *Probing:* How did you update your process after?

---

### üîπ Leadership Principles (7 questions)

11. **Tell me about a time when you took complete ownership of a failing automation pipeline.**
    *Probing:* How did you diagnose and solve the issue?

12. **Describe a situation where you had to learn something completely new to solve a problem.**
    *(‚ÄúLearn and Be Curious‚Äù)*

13. **You‚Äôve worked with undocumented APIs. How do you ensure thorough test coverage in such cases?**
    *(‚ÄúDeep Dive‚Äù)*

14. **Have you mentored or guided junior engineers or AWF team members? What was your approach?**

15. **How do you balance delivering fast automation vs. writing scalable, long-term code?**
    *Probing:* Do you trade quality for speed?

16. **Describe an example where you went beyond your immediate responsibilities to ensure quality.**

17. **When something you automated broke in production, how did you handle it?**
    *Probing:* Did you accept accountability and drive RCA?

---

### üîπ Scenario-Based / Thought Process (3 questions)

18. **You're given a legacy test suite with flaky tests. How would you approach fixing it?**
    *Probing:* Do you use logs? Retry patterns? Root cause?

19. **Imagine you‚Äôre asked to set up automation for a new cloud microservice. What would your roadmap be in the first 30 days?**

20. **How would you ensure test coverage and code quality in a rapidly scaling CI/CD environment?**
    *Probing:* What role does test data, mocking, or versioning play?

---

Would you like me to also create a **scorecard/rubric** for these questions with expected signals for pass/no-pass assessment?
